# üìñ Gu√≠a de Uso del Sistema

## üéØ Flujo de Trabajo del Sistema

### 1. Pipeline en Tiempo Real (Streaming)

#### Iniciar Producer
```powershell
python streaming/producer.py
```

**Qu√© hace:**
- Consulta APIs de Yahoo Finance cada 10 segundos
- Obtiene precios en tiempo real de las acciones configuradas
- Publica datos en Kafka topic `stock-prices`

#### Iniciar Consumer
```powershell
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 streaming/consumer.py
```

**Qu√© hace:**
- Consume datos del topic de Kafka
- Procesa en micro-batches
- Detecta cambios significativos de precio
- Almacena en MongoDB
- Genera alertas cuando hay cambios > 5%

#### Iniciar Sistema de Alertas
```powershell
python streaming/alerts.py
```

**Qu√© hace:**
- Monitorea MongoDB en busca de nuevas alertas
- Env√≠a emails a usuarios suscritos
- Registra historial de alertas enviadas

---

### 2. Pipeline Batch Diario

#### Ejecuci√≥n Manual
```powershell
python batch/daily_aggregation.py
```

#### Con Airflow (Autom√°tico)
```powershell
# Ver estado del DAG
airflow dags list

# Ejecutar manualmente
airflow dags trigger daily_batch_pipeline

# Ver logs
airflow tasks logs daily_batch_pipeline run_daily_aggregation 2026-01-13
```

**Qu√© hace:**
- Se ejecuta autom√°ticamente a las 00:00
- Agrega datos del d√≠a anterior
- Calcula m√©tricas: promedio, m√°ximo, m√≠nimo, volumen
- Calcula indicadores t√©cnicos: SMA, RSI
- Guarda en MySQL
- Limpia datos antiguos de MongoDB (>90 d√≠as)

---

### 3. Pipeline de Noticias

#### Scraping Manual
```powershell
python articles/news_scraper.py
```

#### Enviar Res√∫menes Manual
```powershell
python articles/email_sender.py
```

#### Con Airflow (Autom√°tico)
```powershell
# Ejecutar pipeline de noticias
airflow dags trigger news_pipeline
```

**Qu√© hace:**
- Se ejecuta autom√°ticamente a las 09:00
- Scrapea noticias de Yahoo Finance
- Filtra noticias relevantes
- Guarda en MongoDB
- Env√≠a resumen diario por email a usuarios

---

### 4. Aplicaci√≥n Web

#### Iniciar Flask App
```powershell
python flask_web_app/app.py
```

**Acceder a:** http://localhost:5000

#### Funcionalidades:

**P√°gina Principal:**
- Ver precios en tiempo real
- Ver cambios porcentuales
- Ver volumen de operaciones
- Auto-refresh cada 10 segundos

**Dashboard de Acci√≥n:**
- Acceder: http://localhost:5000/dashboard/AAPL
- Ver m√©tricas detalladas
- Ver gr√°fico hist√≥rico
- Ver noticias recientes

**Registro de Usuario:**
- Acceder: http://localhost:5000/register
- Ingresar nombre y email
- Seleccionar acciones a monitorear
- Configurar umbral de alertas
- Habilitar resumen de noticias

---

## üîî Sistema de Alertas

### Configuraci√≥n de Alertas

#### Por Usuario:
1. Registrarse en la web app
2. Seleccionar acciones
3. Configurar umbral (default: 5%)
4. Habilitar alertas

#### Tipos de Alertas:
- **Cambio de Precio**: Cuando el precio cambia m√°s del umbral
- **Volumen Alto**: Cuando el volumen es inusualmente alto
- **Noticias**: Resumen diario de noticias

#### Ejemplo de Email de Alerta:
```
Asunto: üîî Alerta AAPL: +6.25%

Acci√≥n: AAPL
Precio Actual: $175.50
Cambio: +6.25%
Apertura: $165.00
M√°ximo: $176.00
M√≠nimo: $164.50
Volumen: 52,345,678
```

---

## üìä Dashboards y Visualizaci√≥n

### Grafana Dashboards

#### Configurar MySQL Datasource:
1. Abrir http://localhost:3000
2. Configuration > Data Sources > Add MySQL
3. Host: `localhost:3306`
4. Database: `stock_analytics`
5. User: `root`
6. Password: tu_password

#### Crear Dashboard:
```sql
-- Query para gr√°fico de precios
SELECT 
    date as time,
    close_price as value,
    symbol
FROM daily_aggregates
WHERE symbol = 'AAPL'
ORDER BY date DESC
LIMIT 30
```

### Kibana Logs

#### Configurar Index Pattern:
1. Abrir http://localhost:5601
2. Management > Index Patterns
3. Crear pattern: `stock-system-logs*`
4. Time field: `@timestamp`

#### Ver Logs:
- Discover > Seleccionar index pattern
- Filtrar por level, module, etc.
- Crear visualizaciones

---

## ü§ñ Orquestaci√≥n con Airflow

### DAGs Disponibles:

#### 1. `daily_batch_pipeline`
- **Schedule**: Diario a las 00:00
- **Tareas**:
  1. Agregar datos del d√≠a
  2. Limpiar datos antiguos
  3. Enviar email de confirmaci√≥n

#### 2. `news_pipeline`
- **Schedule**: Diario a las 09:00
- **Tareas**:
  1. Scrapear noticias
  2. Enviar res√∫menes
  3. Notificar completado

#### 3. `maintenance_pipeline`
- **Schedule**: Semanal (domingos 02:00)
- **Tareas**:
  1. Limpiar logs de Elasticsearch
  2. Optimizar tablas MySQL
  3. Compactar colecciones MongoDB
  4. Verificar espacio en disco

### Comandos √ötiles:

```powershell
# Listar DAGs
airflow dags list

# Ver estado de un DAG
airflow dags state daily_batch_pipeline 2026-01-13

# Ejecutar manualmente
airflow dags trigger daily_batch_pipeline

# Pausar/Despausar DAG
airflow dags pause daily_batch_pipeline
airflow dags unpause daily_batch_pipeline

# Ver logs de una tarea
airflow tasks logs daily_batch_pipeline run_daily_aggregation 2026-01-13

# Reintentar tarea fallida
airflow tasks clear daily_batch_pipeline -t run_daily_aggregation -s 2026-01-13 -e 2026-01-13
```

---

## üîç Monitoreo del Sistema

### Ver Logs en Tiempo Real:

```powershell
# Logs de Producer
# Se muestran en consola

# Logs de Consumer
# Se muestran en consola de Spark

# Logs en Elasticsearch
# Ver en Kibana: http://localhost:5601
```

### Verificar Estado de Servicios:

```powershell
# Kafka
.\kafka\bin\windows\kafka-topics.bat --describe --topic stock-prices --bootstrap-server localhost:9092

# MongoDB
python -c "from pymongo import MongoClient; print(MongoClient().stock_market.realtime_prices.count_documents({}))"

# MySQL
mysql -u root -p -e "SELECT COUNT(*) FROM stock_analytics.daily_aggregates"

# Elasticsearch
curl http://localhost:9200/_cat/indices?v
```

---

## üìà Consultas √ötiles

### MongoDB:

```javascript
// √öltimos 10 precios de AAPL
db.realtime_prices.find({symbol: "AAPL"}).sort({timestamp: -1}).limit(10)

// Alertas del d√≠a
db.alert_history.find({
    timestamp: {
        $gte: new Date(new Date().setHours(0,0,0,0))
    }
})

// Usuarios registrados
db.users.find({alerts_enabled: true})
```

### MySQL:

```sql
-- Mejor d√≠a de AAPL
SELECT date, close_price, daily_change_pct
FROM daily_aggregates
WHERE symbol = 'AAPL'
ORDER BY daily_change_pct DESC
LIMIT 1;

-- Promedio semanal
SELECT 
    WEEK(date) as week,
    AVG(close_price) as avg_price,
    SUM(total_volume) as weekly_volume
FROM daily_aggregates
WHERE symbol = 'AAPL'
GROUP BY WEEK(date)
ORDER BY week DESC;

-- Top performers
SELECT symbol, AVG(daily_change_pct) as avg_change
FROM daily_aggregates
WHERE date >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)
GROUP BY symbol
ORDER BY avg_change DESC;
```

---

## üõ†Ô∏è Personalizaci√≥n

### Agregar Nuevas Acciones:

```python
# En config/config.py
STOCKS_TO_MONITOR = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'META', 'NVDA', 'NFLX']
```

### Cambiar Intervalo de Producer:

```python
# En config/config.py
PRODUCER_FETCH_INTERVAL = 30  # segundos
```

### Cambiar Umbral de Alertas:

```python
# En config/config.py
PRICE_CHANGE_THRESHOLD = 3.0  # porcentaje
```

---

## üêõ Debugging

### Producer no obtiene datos:
- Verificar conexi√≥n a internet
- Verificar APIs de Yahoo Finance funcionando
- Revisar rate limiting

### Consumer no procesa:
- Verificar Kafka est√© corriendo
- Verificar topic existe
- Revisar logs de Spark

### Alertas no se env√≠an:
- Verificar configuraci√≥n SMTP
- Verificar usuarios registrados
- Revisar logs del alert service

### Web app no muestra datos:
- Verificar MongoDB tiene datos
- Verificar producer y consumer corriendo
- Revisar logs de Flask

---

## üìû Soporte

Para problemas t√©cnicos, revisar:
1. Logs en consola
2. Logs en Elasticsearch/Kibana
3. Logs de Airflow
4. Estado de servicios (Kafka, MongoDB, MySQL)
