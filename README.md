# ğŸ“ˆ Sistema de AnÃ¡lisis y Alertas de Acciones

Sistema completo de ingenierÃ­a de datos para monitoreo, anÃ¡lisis y alertas del mercado de valores en tiempo real.

## ğŸ—ï¸ Arquitectura del Sistema

```
[API Stocks] â†’ [Kafka Producer] â†’ [Kafka Topic]
                                         â†“
                              [Spark Streaming Consumer]
                                         â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“                    â†“                    â†“
              [MongoDB]            [Amazon S3]           [Alert Service]
                                                              â†“
                                                         [Email Users]

[Daily Batch Job] â†’ [MySQL] â†’ [Grafana Dashboard]
[News Scraper] â†’ [MongoDB] â†’ [Email Summary]
[All Services] â†’ [Elasticsearch] â†’ [Kibana]
[Flask App] â†’ [MongoDB/MySQL] (read-only)
```

## ğŸ› ï¸ Stack TecnolÃ³gico

- **Streaming**: Apache Kafka, Apache Spark Structured Streaming
- **OrquestaciÃ³n**: Apache Airflow
- **Bases de Datos**: MongoDB, MySQL, Elasticsearch
- **Almacenamiento**: Amazon S3
- **VisualizaciÃ³n**: Grafana, Kibana
- **Web**: Flask, Python
- **APIs**: Yahoo Finance, Alpha Vantage

## ğŸ“ Estructura del Proyecto

```
data_project/
â”œâ”€â”€ streaming/          # Pipeline en tiempo real
â”œâ”€â”€ batch/             # Pipeline batch diario
â”œâ”€â”€ articles/          # Pipeline de noticias
â”œâ”€â”€ logs/              # Sistema de logging
â”œâ”€â”€ flask_web_app/     # AplicaciÃ³n web
â”œâ”€â”€ airflow/           # DAGs de Airflow
â”œâ”€â”€ config/            # Configuraciones
â”œâ”€â”€ database/          # Scripts de BD
â”œâ”€â”€ tests/             # Tests unitarios
â””â”€â”€ requirements.txt   # Dependencias
```

## ğŸš€ InstalaciÃ³n

```bash
# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
cp .env.example .env
# Editar .env con tus credenciales
```

## âš™ï¸ ConfiguraciÃ³n

1. **Kafka**: Iniciar Zookeeper y Kafka broker
2. **MongoDB**: Configurar instancia local o cloud
3. **MySQL**: Crear base de datos y tablas
4. **Elasticsearch**: Configurar cluster
5. **AWS S3**: Configurar bucket y credenciales

Ver [SETUP.md](docs/SETUP.md) para instrucciones detalladas.

## ğŸƒ EjecuciÃ³n

### Pipeline en Tiempo Real
```bash
# Iniciar Kafka producer
python streaming/producer.py

# Iniciar Spark consumer
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 streaming/consumer.py
```

### Pipeline Batch
```bash
# EjecuciÃ³n manual
python batch/daily_aggregation.py

# Con Airflow
airflow dags trigger daily_batch_pipeline
```

### AplicaciÃ³n Web
```bash
python flask_web_app/app.py
# Visitar http://localhost:5000
```

## ğŸ“Š Dashboards

- **Grafana**: http://localhost:3000 - MÃ©tricas de mercado
- **Kibana**: http://localhost:5601 - Logs del sistema
- **Flask App**: http://localhost:5000 - Interfaz de usuario

## ğŸ”” Sistema de Alertas

Las alertas se envÃ­an por email cuando:
- Cambio de precio > 5% (configurable)
- Volumen inusualmente alto
- Noticias relevantes detectadas

## ğŸ“ Licencia

MIT License
