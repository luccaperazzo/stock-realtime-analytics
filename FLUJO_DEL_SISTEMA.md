# üìä Flujo del Sistema - Stock Analysis System

## üîÑ Arquitectura General (Docker Compose)

```
[Producer] ‚Üí [Kafka] ‚Üí [Consumer] ‚Üí [MongoDB] ‚Üí [Batch (cron)] ‚Üí [MySQL]
                          ‚Üì            ‚Üì                             ‚Üì
                     [Alertas]    [News Scraper]              [Dashboard Flask]
                          ‚Üì            ‚Üì (cron)                     ‚Üì
                      [Email]      [Email Daily]                [Grafana]

                    [Elasticsearch] ‚Üê [Logs de todos los componentes]
                          ‚Üì
                      [Kibana]
```

**Contenedores Docker:** 13 servicios en docker-compose.yml
- Infraestructura: Zookeeper, Kafka, MongoDB, MySQL, Elasticsearch, Kibana
- Procesamiento: Producer, Consumer, Alerts, Batch (cron), News (cron)
- Visualizaci√≥n: Flask (5000), Grafana (3000)

---

## üìù Flujo Detallado de Datos

### 1Ô∏è‚É£ **Generaci√≥n de Datos en Tiempo Real**

**Archivo:** `streaming/producer.py` (Dockerizado)

**Proceso:**
1. Se conecta a **Alpha Vantage API** para obtener datos reales de acciones (AAPL, GOOGL, MSFT, AMZN, TSLA, META, NVDA)
2. Fallback a datos simulados si alcanza l√≠mite de API (5 llamadas/minuto)
3. Cada 15 segundos captura:
   - Precio actual
   - Volumen
   - Precio de apertura/cierre
   - High/Low del d√≠a
   - Cambio porcentual
4. Serializa los datos a JSON
5. Env√≠a el mensaje al topic `stock-prices` de **Kafka**

**API:** Alpha Vantage (key en .env: BKK5OMNA2MT9CYZC)

**Salida:** Mensajes JSON en Kafka
```json
{
  "symbol": "AAPL",
  "timestamp": "2026-01-13T15:30:00",
  "price": 261.05,
  "volume": 287051,
  "open": 260.22,
  "high": 261.35,
  "low": 260.19,
  "change_percent": 0.32
}
```

---

### 2Ô∏è‚É£ **Distribuci√≥n de Mensajes**

**Servicio:** Apache Kafka + Zookeeper

**Proceso:**
1. Kafka recibe mensajes del producer
2. Los almacena temporalmente en el topic `stock-prices` (3 particiones)
3. Distribuye los mensajes a todos los consumers suscritos
4. Garantiza entrega y orden de mensajes

**Rol:** Message broker / Cola de mensajes distribuida

---

### 3Ô∏è‚É£ **Consumo y Almacenamiento en Tiempo Real**

**Archivo:** `streaming/consumer.py` (Refactorizado - usa kafka-python en lugar de Spark)

**Proceso:**
1. Se suscribe al topic `stock-prices` de Kafka
2. Consume mensajes en tiempo real (consumer group: `stock-consumer-group`)
3. Para cada mensaje:
   - Deserializa el JSON
   - Calcula `price_change_pct` adicional
   - Agrega timestamp de procesamiento
   - Inserta en **MongoDB** (colecci√≥n `realtime_prices`)
4. Log de cada mensaje procesado

**Nota:** Archivo original mantenido pero completamente reescrito sin PySpark (solo kafka-python + pymongo)

**Base de Datos:** MongoDB
- **Database:** `stock_market`
- **Colecci√≥n:** `realtime_prices`
- **Documentos:** 100+ registros en tiempo real

---

### 4Ô∏è‚É£ **Sistema de Alertas en Tiempo Real**

**Archivo:** `streaming/alerts.py` (Dockerizado)

**Proceso:**
1. Tambi√©n consume del topic `stock-prices` de Kafka
2. Analiza cada mensaje en busca de condiciones de alerta:
   - **Cambio de precio > 5%** (configurable en `.env`)
   - **Volumen anormalmente alto** (> 2x promedio)
3. Cuando detecta una alerta:
   - Registra en logs
   - Env√≠a email de notificaci√≥n (requiere credenciales SMTP en .env)
   - Guarda en MySQL (tabla `alert_log`)

**Configuraci√≥n:** `.env`
```
PRICE_CHANGE_THRESHOLD=5.0
VOLUME_THRESHOLD_MULTIPLIER=2.0
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
EMAIL_SENDER=tu_email@gmail.com
EMAIL_PASSWORD=contrase√±a_de_aplicacion
```

---

### 5Ô∏è‚É£ **Procesamiento Batch Diario (Automatizado)**

**Archivo:** `batch/daily_aggregation.py` (Dockerizado con cron)

**Proceso:**
1. Se ejecuta **autom√°ticamente a las 00:00** via cron job en contenedor Docker
2. Lee datos de **MongoDB** del d√≠a actual (o d√≠a anterior)
3. Para cada s√≠mbolo de acci√≥n:
   - Calcula m√©tricas agregadas:
     - Precio de apertura/cierre
     - Precio m√°ximo/m√≠nimo
     - Precio promedio
     - Volumen total y promedio
     - Volatilidad (desviaci√≥n est√°ndar)
     - Cambio porcentual diario
   - Calcula indicadores t√©cnicos:
     - SMA (Simple Moving Average) - 20 per√≠odos
     - RSI (Relative Strength Index)
4. Guarda resultados en **MySQL** (tabla `daily_aggregates`)
5. Limpia datos antiguos de MongoDB (retiene 90 d√≠as)

**Cron Job:** `0 0 * * *` (medianoche diaria)

**Base de Datos:** MySQL
- **Database:** `stock_analytics`
- **Tabla principal:** `daily_aggregates`
- **Puerto:** 3307 (externo), 3306 (interno Docker)
- **Registros:** 1 por d√≠a por acci√≥n (7 acciones monitoreadas)

---

### 6Ô∏è‚É£ **News Scraper & Email Service (Automatizado)**

**Archivos:** `articles/daily_news.py`, `articles/news_scraper.py`, `articles/email_sender.py` (Dockerizado con cron)

**Proceso:**
1. Se ejecuta **autom√°ticamente a las 08:00 AM** via cron job en contenedor Docker
2. **Scraper de noticias:**
   - Busca noticias de Yahoo Finance, Google News, Alpha Vantage
   - Para cada acci√≥n monitoreada (7 s√≠mbolos)
   - Extrae: t√≠tulo, resumen, fuente, fecha, URL
   - Filtra noticias relevantes
   - Almacena en **MongoDB** (colecci√≥n `news_articles`)
3. **Email Service:**
   - Lee usuarios registrados de MongoDB
   - Genera resumen HTML con noticias del d√≠a
   - Env√≠a email a cada usuario
   - Log de emails enviados

**Cron Job:** `0 8 * * *` (08:00 AM diaria)

**Configuraci√≥n SMTP requerida en .env:**
- Gmail: Contrase√±a de aplicaci√≥n (no contrase√±a normal)
- Acceso en: https://myaccount.google.com ‚Üí Seguridad ‚Üí Contrase√±as de aplicaciones

---

### 7Ô∏è‚É£ **Aplicaci√≥n Web (Dashboard)**

**Archivo:** `flask_web_app/app.py` (Dockerizado)

**Proceso:**
1. Aplicaci√≥n Flask corriendo en `http://localhost:5000`
2. **Rutas principales:**
   - `/` - P√°gina principal con precios en tiempo real
   - `/dashboard` - Dashboard con gr√°ficos y an√°lisis
   - `/register` - Registro de usuarios

3. **Fuentes de datos:**
   - **MongoDB** ‚Üí Datos en tiempo real (√∫ltimos precios)
   - **MySQL** ‚Üí Datos agregados y m√©tricas hist√≥ricas
   - **MongoDB (users)** ‚Üí Autenticaci√≥n de usuarios

4. **Plantillas:** `templates/`
   - `base.html` - Layout base
   - `index.html` - P√°gina principal
   - `dashboard.html` - Dashboard de an√°lisis
   - `register.html` - Formulario de registro

**Acceso:** http://localhost:5000

---

### 8Ô∏è‚É£ **Grafana - Visualizaci√≥n Avanzada**

**Servicio:** Grafana (Dockerizado)

**Acceso:** http://localhost:3000
- **Usuario:** admin
- **Contrase√±a:** admin

**Proceso:**
1. Datasource configurado autom√°ticamente:
   - MySQL Stock Analytics (mysql:3306)
   - Conexi√≥n a `stock_analytics` database
2. Dashboard pre-configurado: "Stock Market Analytics Dashboard"
3. **Paneles incluidos:**
   - Stock Price Trends (30 d√≠as)
   - Trading Volume (30 d√≠as)
   - Daily Change % (gauges)
   - RSI Indicator (30 d√≠as)
   - Stock Performance Summary (tabla)
4. Auto-refresh cada 30 segundos

**Directorio de configuraci√≥n:** `grafana/provisioning/`
- `datasources/mysql.yml` - Conexi√≥n a MySQL
- `dashboards/stock-analytics.json` - Dashboard principal

---

### 9Ô∏è‚É£ **Sistema de Logs Centralizado**

**Archivo:** `logs/logger_config.py`

**Proceso:**
1. Todos los componentes del sistema usan el logger centralizado
2. Cada log se env√≠a a:
   - **Consola** (stdout) ‚Üí Para desarrollo
   - **Elasticsearch** ‚Üí Para almacenamiento y b√∫squeda
3. Estructura de logs:
   ```json
   {
     "timestamp": "2026-01-13T15:30:00",
     "level": "INFO",
     "logger": "kafka_producer",
     "message": "Mensaje enviado a Kafka",
     "module": "producer",
     "function": "send_message",
     "line": 45
   }
   ```

**Elasticsearch:**
- **√çndice:** `stock-system-logs`
- **Host:** localhost:9200

---

### üîü **Visualizaci√≥n de Logs**

**Servicio:** Kibana (Dockerizado)

**Acceso:** http://localhost:5601

**Configuraci√≥n:**
- √çndice: `stock-system-logs`
- Conectado a Elasticsearch (elasticsearch:9200)
- 260+ log entries de todos los servicios

**Funcionalidad:**
- B√∫squeda de logs en tiempo real
- Filtros por nivel (INFO, WARNING, ERROR)
- Filtros por componente (producer, consumer, batch, etc.)
- Visualizaciones y dashboards de logs
- Detecci√≥n de errores y patrones

---

## üóÇÔ∏è Estructura de Bases de Datos

### MongoDB (Datos en Tiempo Real)
```
stock_market/
‚îú‚îÄ‚îÄ realtime_prices     ‚Üí Datos de streaming (100+ docs)
‚îú‚îÄ‚îÄ news_articles       ‚Üí Art√≠culos scrapeados (diarios)
‚îú‚îÄ‚îÄ users              ‚Üí Usuarios de la app Flask
‚îî‚îÄ‚îÄ alert_history      ‚Üí Historial de alertas
```

### MySQL (Datos Agregados)
```
stock_analytics/
‚îú‚îÄ‚îÄ daily_aggregates    ‚Üí Agregados diarios (7 acciones)
‚îú‚îÄ‚îÄ weekly_aggregates   ‚Üí Res√∫menes semanales
‚îú‚îÄ‚îÄ stock_performance   ‚Üí M√©tricas de performance
‚îî‚îÄ‚îÄ alert_log          ‚Üí Log de alertas enviadas
```

### Elasticsearch (Logs)
```
stock-system-logs/     ‚Üí Logs de todos los componentes
```

---

## üîß Componentes Opcionales (No Configurados)

### AWS S3
**Archivo:** `streaming/consumer.py` (m√©todo deshabilitado)
- Archivar√≠a datos hist√≥ricos en formato Parquet
- Particionado por s√≠mbolo de acci√≥n
- Para an√°lisis a largo plazo

### Apache Airflow
**Archivos:** `airflow/dags/` (no dockerizados)
- `daily_batch_dag.py` ‚Üí Orquestaci√≥n del batch diario
- `news_pipeline_dag.py` ‚Üí Pipeline de scraping de noticias
- `maintenance_dag.py` ‚Üí Tareas de mantenimiento

**Nota:** Sistema usa cron jobs en Docker en lugar de Airflow

---

## üìä Ejemplo de Flujo Completo

### Escenario: Nueva actualizaci√≥n de precio de AAPL

**T+0s:** Producer obtiene precio de AAPL ($260.21)
```
streaming/producer.py ‚Üí yfinance API
```

**T+0.1s:** Env√≠o a Kafka
```
Producer ‚Üí Kafka (topic: stock-prices)
```

**T+0.2s:** Consumer procesa el mensaje
```
Kafka ‚Üí consumer_simple.py ‚Üí MongoDB (realtime_prices)
Log: "Mensaje procesado - Symbol: AAPL, Price: $260.21"
```

**T+0.2s:** Sistema de alertas verifica condiciones
```
Kafka ‚Üí alerts_simple.py
Verifica: ¬øCambio > 5%? NO
Verifica: ¬øVolumen alto? NO
‚Üí No se env√≠a alerta
```

**T+0.3s:** Flask actualiza dashboard
```
MongoDB ‚Üí Flask App ‚Üí Usuario ve $260.21 en tiempo real
```

**T+0.3s:** Logs enviados a Elasticsearch
```
logger ‚Üí Elasticsearch (stock-system-logs)
‚Üí Disponible en Kibana
```

**T+24h:** Batch nocturno procesa el d√≠a
```
batch/daily_aggregation.py:
1. Lee todos los registros de AAPL del d√≠a desde MongoDB
2. Calcula: open=$260.53, close=$260.21, avg=$260.37
3. Calcula: volatility=0.15, change%=-0.12%
4. Guarda en MySQL (daily_aggregates)
5. Disponible en Flask Dashboard hist√≥rico
```

---

## üöÄ Orden de Inicio del Sistema (Docker Compose)

**Comando √∫nico:** `docker-compose up -d`

**Orden autom√°tico de dependencias:**
1. **Zookeeper** (prerequisito de Kafka)
2. **Kafka** (message broker) - healthcheck activado
3. **MongoDB** (base de datos NoSQL) - healthcheck activado
4. **MySQL** (base de datos SQL) - healthcheck activado
5. **Elasticsearch** (motor de logs) - healthcheck activado
6. **Kibana** (visualizaci√≥n de logs)
7. **Producer** (depende de Kafka)
8. **Consumer** (depende de Kafka + MongoDB)
9. **Alerts** (depende de Kafka)
10. **Flask App** (depende de MongoDB + MySQL)
11. **Batch** (depende de MongoDB + MySQL) - cron 00:00
12. **News** (depende de MongoDB) - cron 08:00
13. **Grafana** (depende de MySQL)

**Verificar estado:** `docker ps`
**Ver logs:** `docker-compose logs -f [servicio]`
**Reiniciar todo:** `docker-compose restart`
**Detener todo:** `docker-compose down`

---

## üìÅ Archivos Clave por Componente

### Streaming
- `streaming/producer.py` - Producer con Alpha Vantage API ‚úÖ (Dockerizado)
- `streaming/consumer.py` - Consumer refactorizado (kafka-python) ‚úÖ (Dockerizado)
- `streaming/alerts.py` - Sistema de alertas ‚úÖ (Dockerizado)

### Batch
- `batch/daily_aggregation.py` - Procesamiento diario ‚úÖ (Dockerizado + cron 00:00)

### News & Email
- `articles/daily_news.py` - Script principal ‚úÖ (Dockerizado + cron 08:00)
- `articles/news_scraper.py` - Scraper de noticias ‚úÖ
- `articles/email_sender.py` - Env√≠o de emails ‚úÖ

### Web
- `flask_web_app/app.py` - Aplicaci√≥n Flask ‚úÖ (Dockerizado)
- `flask_web_app/templates/*.html` - Plantillas HTML ‚úÖ

### Configuraci√≥n
- `.env` - Variables de entorno ‚úÖ
- `config/config.py` - Configuraci√≥n centralizada ‚úÖ
- `docker-compose.yml` - Orquestaci√≥n de 13 servicios ‚úÖ
- `Dockerfile.producer` - Imagen del producer ‚úÖ
- `Dockerfile.consumer` - Imagen del consumer ‚úÖ
- `Dockerfile.alerts` - Imagen de alertas ‚úÖ
- `Dockerfile.flask` - Imagen de Flask ‚úÖ
- `Dockerfile.batch` - Imagen de batch con cron ‚úÖ
- `Dockerfile.news` - Imagen de news con cron ‚úÖ

### Logs
- `logs/logger_config.py` - Sistema de logging ‚úÖ

### Base de Datos
- `database/mysql_schema.sql` - Schema de MySQL ‚úÖ
- `database/setup_mongodb.py` - Setup de MongoDB

### Grafana
- `grafana/provisioning/datasources/mysql.yml` - Datasource ‚úÖ
- `grafana/provisioning/dashboards/stock-analytics.json` - Dashboard ‚úÖ

---

## ‚úÖ Estado Actual del Sistema

### Componentes Activos (Docker Compose)
- ‚úÖ Zookeeper + Kafka (streaming)
- ‚úÖ Producer con Alpha Vantage API (datos reales + fallback simulado)
- ‚úÖ Consumer refactorizado (kafka-python, sin Spark)
- ‚úÖ MongoDB (100+ documentos en realtime_prices)
- ‚úÖ MySQL puerto 3307 (7 acciones en daily_aggregates)
- ‚úÖ Batch processing **automatizado** (cron 00:00 diaria)
- ‚úÖ News Scraper + Email **automatizado** (cron 08:00 diaria)
- ‚úÖ Flask App (http://localhost:5000)
- ‚úÖ Sistema de alertas (requiere SMTP configurado)
- ‚úÖ Elasticsearch + Kibana (http://localhost:5601, 260+ logs)
- ‚úÖ Grafana (http://localhost:3000, dashboards pre-configurados)

### Total: 13 contenedores Docker

### Componentes Pendientes
- ‚è≥ AWS S3 (archivado hist√≥rico)
- ‚è≥ Airflow (reemplazado por cron jobs)
- ‚è≥ Configuraci√≥n SMTP para emails (requiere credenciales de usuario)

---

## üîç Monitoreo del Sistema

### Verificar que todo funcione (Docker):

**Ver todos los contenedores:**
```bash
docker ps
```

**Ver logs de un servicio:**
```bash
docker-compose logs -f producer
docker-compose logs -f consumer
docker-compose logs -f batch
docker-compose logs -f news
```

**Acceder a servicios web:**
- **Flask Dashboard**: http://localhost:5000
- **Grafana Analytics**: http://localhost:3000 (admin/admin)
- **Kibana Logs**: http://localhost:5601

### Ver datos directamente:

**MongoDB (desde contenedor):**
```bash
docker exec -it mongodb mongosh
use stock_market
db.realtime_prices.countDocuments()
db.realtime_prices.find().limit(5)
db.news_articles.countDocuments()
```

**MySQL (desde contenedor):**
```bash
docker exec -it mysql mysql -uroot -ppascualina stock_analytics
SELECT * FROM daily_aggregates ORDER BY date DESC LIMIT 10;
SELECT symbol, avg_price, daily_change_pct FROM daily_aggregates WHERE date = CURDATE();
```

**Elasticsearch:**
```bash
curl http://localhost:9200/_cat/indices
curl http://localhost:9200/stock-system-logs/_count
```

### Ejecutar procesos manualmente:

**Batch diario:**
```bash
docker exec batch python batch/daily_aggregation.py
```

**News scraper:**
```bash
docker exec news python articles/daily_news.py
```

---

## üìû Flujo de Alertas

```
[Kafka] ‚Üí [alerts_simple.py]
              ‚Üì
    ¬øPrecio cambi√≥ > 5%?
              ‚Üì S√ç
    [Registra en MySQL]
              ‚Üì
    [Env√≠a Email] (si configurado)
              ‚Üì
    [Log en Elasticsearch]
```

**Configuraci√≥n de Email** (`.env`):
```
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
EMAIL_SENDER=tu_email@gmail.com
EMAIL_PASSWORD=tu_app_password
```

---

Este es el flujo completo del sistema Stock Analysis System. Todos los componentes trabajan juntos para proporcionar an√°lisis en tiempo real y agregado de datos del mercado de valores.
